
---

## ðŸ“ Project Structure

```

project_root/
â”œâ”€â”€ datasets/                    # Dataset & preprocessing
â”‚   â”œâ”€â”€ multimodal_gaze_dataset.py
â”‚   â””â”€â”€ preprocess_vgs.py
â”œâ”€â”€ feature_extraction/          # Feature extraction scripts
â”‚   â”œâ”€â”€ audio
â”‚   â”‚   â”œâ”€â”€ extract_egemaps.py
â”‚   â”‚   â”œâ”€â”€ extract_wav2vec.py
â”‚   â”‚   â””â”€â”€ process_audio.py
â”‚   â”œâ”€â”€ semantic
â”‚   â”‚   â”œâ”€â”€ extract_mclip.py
â”‚   â”‚   â”œâ”€â”€ extract_whisperx.py
â”‚   â”‚   â””â”€â”€ extract_xlmr.py
â”‚   â””â”€â”€ visual
â”‚       â”œâ”€â”€ extract_clip.py
â”‚       â”œâ”€â”€ extract_dino.py
â”‚       â””â”€â”€ preprocess_images.py
â”œâ”€â”€ models/                      # Model definitions
â”‚   â”œâ”€â”€ multimodal_transformer.py
â”‚   â”œâ”€â”€ gaze_head.py
â”‚   â””â”€â”€ full_model.py
â”œâ”€â”€ utils/                       # Tools: loss, metrics, vis
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ configs/                     # YAML config files
â”‚   â””â”€â”€ multimodal.yaml
â”œâ”€â”€ outputs/                     # Checkpoints, logs, visual output
â”œâ”€â”€ train.py                     # Training pipeline
â”œâ”€â”€ evaluate.py                  # Evaluation script
â”œâ”€â”€ README.md
â””â”€â”€ environment.yml

```


---

## ðŸš§ Project Development TODO

### ðŸ› ï¸ Feature Extraction

* \[âœ“] **Audio:** eGeMAPS and Wav2Vec2.0 extraction, windowed features saved as `.npy`
* \[âœ“] **Visual:** CLIP/DINOv2 features extraction, pre-processing, saving standardized features
* \[âœ“] **Semantic:** WhisperX transcription, XLM-R/M-CLIP embeddings, saving text features
* \[âœ“] **Multimodal Alignment:**
  Scripts for precise time/frame alignment of all modalities and ground truth labels
  *â€” All features and labels are aligned using the same sliding window parameters:*
  Â Â Â Â â€¢ Frame rate: **25 fps** (0.04 s per frame)
  Â Â Â Â â€¢ Window size: **1.0 s** (25 frames)
  Â Â Â Â â€¢ Step: **0.04 s** (1 frame)

---

### ðŸ¤ Fusion Model

* \[âœ“] **Multi-modal Transformer fusion** (visual/audio/semantic token input)
* [ ] **Group-level attention features:** Add "who looks at whom" social tokens for advanced social gaze modeling
* [ ] **Cross-modal attention visualization and interpretability tools:** Visualize attention maps and improve model explainability

---

### ðŸš¦ Training & Evaluation

* \[âœ“] **Basic train/eval pipeline** using aligned features
* [ ] **Batch/multi-clip training & efficient dataloading**
* [ ] **Evaluation metrics:** Add L2 distance, AUC, and other relevant metrics
* [ ] **Baseline & ablation experiments:** Compare with MMGaze, GazeLLE, and other published approaches

---

### ðŸ“š Docs & Utilities

* \[âœ“] **Main README and example usage**
* [ ] **Code comments and per-script usage docs**
* [ ] **Config templates and demo visualizations**

---

## ðŸš€ Getting Started

```bash
# 1. Install dependencies
conda env create -f environment.yml
conda activate gazelle

# 2. Extract audio features
python feature_extraction/audio/process_audio.py \
  --input_audio_dir data/audio \
  --output_dir data/audio_feat

# 3. Extract text features (WhisperX â†’ XLM-R + M-CLIP)
python feature_extraction/semantic/extract_whisperx.py \
  --input_audio_dir data/audio \
  --output_dir data/text_feat

# 4. Extract visual features (optional if using CLIP)
python feature_extraction/visual/extract_clip.py \
  --input_dir data/frames \
  --output_npy data/visual_feat.npy \
  --model_name 'ViT-B-16-plus-240' \
  --pretrained 'laion400m_e32'

# 5. Train the model
python train.py --config configs/multimodal.yaml

# 6. Evaluate the model
python evaluate.py --checkpoint outputs/best_model.pth
```

---

## ðŸ› ï¸ Required Python Packages & Frameworks

### Core ML Framework

* `torch` (>= 2.6)
* `torchvision`
* `torchaudio`
* `transformers` (for Wav2Vec2, XLM-R, M-CLIP)
* `openai-whisper` / `whisperx`
* `open-clip-torch`
* `pytorch-lightning`
* `jsonargparse`

### Audio Processing

* `librosa`
* `opensmile`

### Visual Embedding

* `CLIP` (via `open-clip-torch` or `clip` from OpenAI)

### Data Handling / Utilities

* `numpy`
* `pandas`
* `tqdm`
* `scikit-learn`
* `timm`
* `matplotlib`
* `sentence-transformers`
* `sentencepiece`
* `opencv-python`
* `omegaconf` or `PyYAML` (for config management)

---
