A PyTorch-based implementation of audio-visual-semantic gaze target prediction. This project builds upon the [GazeLLE](https://github.com/facebookresearch/GazeLLE) framework and extends it by incorporating synchronized audio and textual semantics (e.g., from WhisperX + XLM-R + CLIP) for predicting human gaze targets in real-world dialogues.

---

## 📁 Project Structure

```

project_root/
├── datasets/                    # Dataset & preprocessing
│   ├── multimodal_gaze_dataset.py
│   └── preprocess_vgs.py
├── feature_extraction/          # Feature extraction scripts
│   ├── audio
│   │   ├── extract_egemaps.py
│   │   ├── extract_wav2vec.py
│   │   └── process_audio.py
│   ├── semantic
│   │   ├── extract_mclip.py
│   │   ├── extract_whisperx.py
│   │   └── extract_xlmr.py
│   └── visual
│       ├── extract_clip.py
│       ├── extract_dino.py
│       └── preprocess_images.py
├── models/                      # Model definitions
│   ├── multimodal_transformer.py
│   ├── gaze_head.py
│   └── full_model.py
├── utils/                       # Tools: loss, metrics, vis
│   ├── losses.py
│   ├── metrics.py
│   └── visualization.py
├── configs/                     # YAML config files
│   └── multimodal.yaml
├── outputs/                     # Checkpoints, logs, visual output
├── train.py                     # Training pipeline
├── evaluate.py                  # Evaluation script
├── README.md
└── environment.yml

```

---



## 🚀 Getting Started

```bash
# 1. Install dependencies
conda env create -f environment.yml
conda activate gazelle

# 2. Extract audio features
python feature_extraction/audio/process_audio.py \
  --input_audio_dir data/audio \
  --output_dir data/audio_feat

# 3. Extract text features (WhisperX → XLM-R + M-CLIP)
python feature_extraction/semantic/extract_whisperx.py \
  --input_audio_dir data/audio \
  --output_dir data/text_feat

# 4. Extract visual features (optional if using CLIP)
python feature_extraction/visual/extract_clip.py \
  --input_dir data/frames \
  --output_npy data/visual_feat.npy \
  --model_name 'ViT-B-16-plus-240' \
  --pretrained 'laion400m_e32'

# 5. Train the model
python train.py --config configs/multimodal.yaml

# 6. Evaluate the model
python evaluate.py --checkpoint outputs/best_model.pth
```

---

## 🛠️ Required Python Packages & Frameworks

### Core ML Framework

* `torch` (>= 2.6)
* `torchvision`
* `torchaudio`
* `transformers` (for Wav2Vec2, XLM-R, M-CLIP)
* `openai-whisper` / `whisperx`
* `open-clip-torch`
* `pytorch-lightning`
* `jsonargparse`

### Audio Processing

* `librosa`
* `opensmile`

### Visual Embedding

* `CLIP` (via `open-clip-torch` or `clip` from OpenAI)

### Data Handling / Utilities

* `numpy`
* `pandas`
* `tqdm`
* `scikit-learn`
* `timm`
* `matplotlib`
* `sentence-transformers`
* `sentencepiece`
* `opencv-python`
* `omegaconf` or `PyYAML` (for config management)

---
