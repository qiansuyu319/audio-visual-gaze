A PyTorch-based implementation of audio-visual-semantic gaze target prediction. This project builds upon the [GazeLLE](https://github.com/facebookresearch/GazeLLE) framework and extends it by incorporating synchronized audio and textual semantics (e.g., from WhisperX + XLM-R + CLIP) for predicting human gaze targets in real-world dialogues.

---

## 📁 Project Structure

```

project_root/
├── datasets/                    # Dataset & preprocessing
│   ├── multimodal_gaze_dataset.py
│   └── preprocess_vgs.py
├── feature_extraction/          # Feature extraction scripts
│   ├── audio
│   │   ├── extract_egemaps.py
│   │   ├── extract_wav2vec.py
│   │   └── process_audio.py
│   ├── semantic
│   │   ├── extract_mclip.py
│   │   ├── extract_whisperx.py
│   │   └── extract_xlmr.py
│   └── visual
│       ├── extract_clip.py
│       ├── extract_dino.py
│       └── preprocess_images.py
├── models/                      # Model definitions
│   ├── multimodal_transformer.py
│   ├── gaze_head.py
│   └── full_model.py
├── utils/                       # Tools: loss, metrics, vis
│   ├── losses.py
│   ├── metrics.py
│   └── visualization.py
├── configs/                     # YAML config files
│   └── multimodal.yaml
├── outputs/                     # Checkpoints, logs, visual output
├── train.py                     # Training pipeline
├── evaluate.py                  # Evaluation script
├── README.md
└── environment.yml

```

---

## ✅ Implementation Plan (TODO)

### 🥇 STEP 1: Feature Extraction

- [ ] `extract_audio_features.py`: Wav2Vec2 + eGeMAPS (via DISCOVER)  
- [ ] `extract_text_features.py`: WhisperX → transcript → XLM-R & M-CLIP  
- [ ] Output: one `.pt` file per frame with aligned features

### 🥈 STEP 2: Dataset

- [ ] Extend `GazeDataset` to return:
  ```python
  {
    image, audio_feat, text_feat,
    gaze_heatmap, inout_label
  }
```

* [ ] Align all `.pt` features per frame index

### 🥉 STEP 3: Model Bootstrap

* [ ] Build `MultimodalGazelleModel` with dummy outputs
* [ ] Ensure model input/output interface is valid
* [ ] Connect losses and train loop (MSE + BCE)

---

## 🏗️ Full Model Design

* 🔹 `audio_proj = nn.Linear(audio_dim, d_model)`
* 🔹 `text_proj = nn.Linear(text_dim, d_model)`
* 🔹 `visual_proj = nn.Linear(clip_dim, d_model)`
* 🔹 `transformer(fused_feat)`
* 🔹 `gaze_head` → heatmap output
* 🔹 `cls_token` → in/out prediction

---

## 📊 Evaluation

* L2 Distance (normalized)
* In/Out Frame Classification Accuracy
* Optional: AUC, hit rate

---

## 🌱 Optional Enhancements

* [ ] Modal dropout (training-time masking)
* [ ] Gated fusion / FiLM / cross-attention fusion
* [ ] Fine-tune CLIP/XLM-R encoders (resource-dependent)

---

## 🚀 Getting Started

```bash
# 1. Install dependencies
conda env create -f environment.yml
conda activate gazelle

# 2. Run the full feature extraction pipeline
python main.py --input_dir data --output_dir processed_data
```

---

## 🛠️ Required Python Packages & Frameworks

### Core ML Framework

* `torch` (>= 2.6)
* `torchvision`
* `torchaudio`
* `transformers` (for Wav2Vec2, XLM-R, M-CLIP)
* `openai-whisper` / `whisperx`
* `open-clip-torch`
* `pytorch-lightning`
* `jsonargparse`

### Audio Processing

* `librosa`
* `opensmile`

### Visual Embedding

* `CLIP` (via `open-clip-torch` or `clip` from OpenAI)

### Data Handling / Utilities

* `numpy`
* `pandas`
* `tqdm`
* `scikit-learn`
* `timm`
* `matplotlib`
* `sentence-transformers`
* `sentencepiece`
* `opencv-python`
* `omegaconf` or `PyYAML` (for config management)

---

## 📌 Notes

* Begin with feature extraction and validate sample alignment.
* Use dummy model output to debug the training pipeline before real modeling.
* Modularize everything so you can test audio-only or text-only branches independently.

---
