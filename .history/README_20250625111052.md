A PyTorch-based implementation of audio-visual-semantic gaze target prediction. This project builds upon the [GazeLLE](https://github.com/facebookresearch/GazeLLE) framework and extends it by incorporating synchronized audio and textual semantics (e.g., from WhisperX + XLM-R + CLIP) for predicting human gaze targets in real-world dialogues.

---

## ðŸ“ Project Structure

```

project_root/
â”œâ”€â”€ datasets/                    # Dataset & preprocessing
â”‚   â”œâ”€â”€ multimodal_gaze_dataset.py
â”‚   â””â”€â”€ preprocess_vgs.py
â”œâ”€â”€ feature_extraction/          # Feature extraction scripts
â”‚   â”œâ”€â”€ audio
â”‚   â”‚   â”œâ”€â”€ extract_egemaps.py
â”‚   â”‚   â”œâ”€â”€ extract_wav2vec.py
â”‚   â”‚   â””â”€â”€ process_audio.py
â”‚   â”œâ”€â”€ semantic
â”‚   â”‚   â”œâ”€â”€ extract_mclip.py
â”‚   â”‚   â”œâ”€â”€ extract_whisperx.py
â”‚   â”‚   â””â”€â”€ extract_xlmr.py
â”‚   â””â”€â”€ visual
â”‚       â”œâ”€â”€ extract_clip.py
â”‚       â”œâ”€â”€ extract_dino.py
â”‚       â””â”€â”€ preprocess_images.py
â”œâ”€â”€ models/                      # Model definitions
â”‚   â”œâ”€â”€ multimodal_transformer.py
â”‚   â”œâ”€â”€ gaze_head.py
â”‚   â””â”€â”€ full_model.py
â”œâ”€â”€ utils/                       # Tools: loss, metrics, vis
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ configs/                     # YAML config files
â”‚   â””â”€â”€ multimodal.yaml
â”œâ”€â”€ outputs/                     # Checkpoints, logs, visual output
â”œâ”€â”€ train.py                     # Training pipeline
â”œâ”€â”€ evaluate.py                  # Evaluation script
â”œâ”€â”€ README.md
â””â”€â”€ environment.yml

```

---

## âœ… Implementation Plan (TODO)

### ðŸ¥‡ STEP 1: Feature Extraction

- [ ] `extract_audio_features.py`: Wav2Vec2 + eGeMAPS (via DISCOVER)  
- [ ] `extract_text_features.py`: WhisperX â†’ transcript â†’ XLM-R & M-CLIP  
- [ ] Output: one `.pt` file per frame with aligned features

### ðŸ¥ˆ STEP 2: Dataset

- [ ] Extend `GazeDataset` to return:
  ```python
  {
    image, audio_feat, text_feat,
    gaze_heatmap, inout_label
  }
```

* [ ] Align all `.pt` features per frame index

### ðŸ¥‰ STEP 3: Model Bootstrap

* [ ] Build `MultimodalGazelleModel` with dummy outputs
* [ ] Ensure model input/output interface is valid
* [ ] Connect losses and train loop (MSE + BCE)

---

## ðŸ—ï¸ Full Model Design

* ðŸ”¹ `audio_proj = nn.Linear(audio_dim, d_model)`
* ðŸ”¹ `text_proj = nn.Linear(text_dim, d_model)`
* ðŸ”¹ `visual_proj = nn.Linear(clip_dim, d_model)`
* ðŸ”¹ `transformer(fused_feat)`
* ðŸ”¹ `gaze_head` â†’ heatmap output
* ðŸ”¹ `cls_token` â†’ in/out prediction

---

## ðŸ“Š Evaluation

* L2 Distance (normalized)
* In/Out Frame Classification Accuracy
* Optional: AUC, hit rate

---

## ðŸŒ± Optional Enhancements

* [ ] Modal dropout (training-time masking)
* [ ] Gated fusion / FiLM / cross-attention fusion
* [ ] Fine-tune CLIP/XLM-R encoders (resource-dependent)

---

## ðŸš€ Getting Started

```bash
# 1. Install dependencies
conda env create -f environment.yml
conda activate gazelle

# 2. Extract audio features
python feature_extraction/audio/process_audio.py \
  --input_audio_dir data/audio \
  --output_dir data/audio_feat

# 3. Extract text features (WhisperX â†’ XLM-R + M-CLIP)
python feature_extraction/semantic/extract_whisperx.py \
  --input_audio_dir data/audio \
  --output_dir data/text_feat

# 4. Extract visual features (optional if using CLIP)
python feature_extraction/visual/extract_clip.py \
  --input_dir data/frames \
  --output_npy data/visual_feat.npy \
  --model_name 'ViT-B-16-plus-240' \
  --pretrained 'laion400m_e32'

# 5. Train the model
python train.py --config configs/multimodal.yaml

# 6. Evaluate the model
python evaluate.py --checkpoint outputs/best_model.pth
```

---

## ðŸ› ï¸ Required Python Packages & Frameworks

### Core ML Framework

* `torch` (>= 2.6)
* `torchvision`
* `torchaudio`
* `transformers` (for Wav2Vec2, XLM-R, M-CLIP)
* `openai-whisper` / `whisperx`
* `open-clip-torch`
* `pytorch-lightning`
* `jsonargparse`

### Audio Processing

* `librosa`
* `opensmile`

### Visual Embedding

* `CLIP` (via `open-clip-torch` or `clip` from OpenAI)

### Data Handling / Utilities

* `numpy`
* `pandas`
* `tqdm`
* `scikit-learn`
* `timm`
* `matplotlib`
* `sentence-transformers`
* `sentencepiece`
* `opencv-python`
* `omegaconf` or `PyYAML` (for config management)

---

## ðŸ“Œ Notes

* Begin with feature extraction and validate sample alignment.
* Use dummy model output to debug the training pipeline before real modeling.
* Modularize everything so you can test audio-only or text-only branches independently.

---
